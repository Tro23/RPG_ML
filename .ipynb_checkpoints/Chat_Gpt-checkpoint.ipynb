{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b93e43-2136-42e2-98c9-02c0c6438774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "\n",
    "# Block 1: Dataset Generation\n",
    "def generate_dataset():\n",
    "    np.random.seed(42)\n",
    "    data = pd.DataFrame({\n",
    "        'Feature_A': np.random.rand(100) * 100,\n",
    "        'Feature_B': np.random.rand(100) * 100 + 10,\n",
    "        'Feature_C': np.random.rand(100) * 100 - 5,\n",
    "        'Target': np.random.choice([0, 1], 100)\n",
    "    })\n",
    "    data['Feature_B'] += data['Feature_A'] * 0.8  # Correlation\n",
    "    return data\n",
    "\n",
    "# Block 2: PyTorch Model Definition and Training\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 64)\n",
    "        self.layer2 = torch.nn.Linear(64, 32)\n",
    "        self.output = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return torch.sigmoid(self.output(x))\n",
    "\n",
    "# Block 3: Optimizers and Loss Functions\n",
    "def get_optimizer_list(model, lr=0.001):\n",
    "    optimizers = {\n",
    "        \"Adam\": torch.optim.Adam(model.parameters(), lr=lr),\n",
    "        \"SGD\": torch.optim.SGD(model.parameters(), lr=lr),\n",
    "        \"RMSprop\": torch.optim.RMSprop(model.parameters(), lr=lr),\n",
    "        \"Adagrad\": torch.optim.Adagrad(model.parameters(), lr=lr),\n",
    "    }\n",
    "    return optimizers\n",
    "\n",
    "def get_loss_functions():\n",
    "    return {\n",
    "        \"Binary Cross Entropy\": torch.nn.BCELoss(),\n",
    "        \"Mean Squared Error\": torch.nn.MSELoss(),\n",
    "        \"Cross Entropy Loss\": torch.nn.CrossEntropyLoss(),\n",
    "    }\n",
    "\n",
    "# Block 4: RL Agent Actions\n",
    "def agent_action(data, action, params, model):\n",
    "    feedback = \"\"\n",
    "    reward = 0  # Initialize reward\n",
    "\n",
    "    if action == \"choose_optimizer\":\n",
    "        optimizers = get_optimizer_list(model, lr=params['learning_rate'])\n",
    "        chosen_optimizer_name = random.choice(list(optimizers.keys()))\n",
    "        chosen_optimizer = optimizers[chosen_optimizer_name]\n",
    "        feedback = f\"Chose {chosen_optimizer_name} optimizer.\"\n",
    "        reward = 5  # Reward for exploring optimizers\n",
    "    elif action == \"choose_loss_function\":\n",
    "        loss_functions = get_loss_functions()\n",
    "        chosen_loss_name = random.choice(list(loss_functions.keys()))\n",
    "        chosen_loss_function = loss_functions[chosen_loss_name]\n",
    "        feedback = f\"Chose {chosen_loss_name} loss function.\"\n",
    "        reward = 5  # Reward for exploring loss functions\n",
    "    elif action == \"apply_pca\":\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(data.drop(columns=['Target']))\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_data = pca.fit_transform(scaled_data)\n",
    "        data = pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\n",
    "        data['Target'] = data.index.map(data['Target'])\n",
    "        feedback = \"Applied PCA, reduced features to 2 principal components.\"\n",
    "        reward = 5  # Reward for reducing dimensionality\n",
    "    elif action == \"adjust_split\":\n",
    "        params['test_size'] = random.uniform(0.1, 0.4)\n",
    "        feedback = f\"Adjusted test/train split to {params['test_size']:.2f} for better evaluation.\"\n",
    "        reward = 3  # Small reward for exploration\n",
    "    return data, feedback, params, reward\n",
    "\n",
    "# Block 5: LLM Query (Using Hugging Face Llama)\n",
    "def query_llm(prompt):\n",
    "    model_name = \"huggingface/llama-7b\"  # Llama model with 7 billion parameters\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=200)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Block 6: Training Model with Optimizer and Loss Function Choices\n",
    "def train_model(data, test_size=0.2, learning_rate=0.001, epsilon=1e-4, optimizer=None, loss_fn=None):\n",
    "    X = data.drop(columns=['Target']).values\n",
    "    y = data['Target'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    model = SimpleNN(X_train.shape[1])\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = loss_fn(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < epsilon:\n",
    "            break\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        predicted = (output > 0.5).float()\n",
    "        accuracy = (predicted == y_test).float().mean().item()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Block 7: RL Loop\n",
    "def rl_loop():\n",
    "    data = generate_dataset()\n",
    "    print(\"Initial Dataset:\\n\", data.head())\n",
    "\n",
    "    actions = [\"choose_optimizer\", \"choose_loss_function\", \"apply_pca\", \"adjust_split\"]\n",
    "    params = {'test_size': 0.2, 'learning_rate': 0.001, 'epsilon': 1e-4}\n",
    "\n",
    "    reward_history = []\n",
    "    accuracy_history = []\n",
    "\n",
    "    for step in range(10):  # Simulate 10 RL steps\n",
    "        action = random.choice(actions)\n",
    "        print(f\"Step {step + 1}: Agent chose action: {action}\")\n",
    "        \n",
    "        data, feedback, params, reward = agent_action(data, action, params, model=None)\n",
    "\n",
    "        print(\"Action Feedback:\", feedback)\n",
    "        llm_response = query_llm(f\"Explain the impact of this action: {feedback}\")\n",
    "        print(\"LLM Response:\", llm_response)\n",
    "\n",
    "        optimizer = None\n",
    "        loss_fn = None\n",
    "        if \"optimizer\" in action:\n",
    "            optimizer = get_optimizer_list(None, lr=params['learning_rate'])[random.choice(list(get_optimizer_list(None).keys()))]\n",
    "        if \"loss_function\" in action:\n",
    "            loss_fn = get_loss_functions()[random.choice(list(get_loss_functions().keys()))]\n",
    "\n",
    "        accuracy = train_model(data, test_size=params['test_size'], learning_rate=params['learning_rate'], epsilon=params['epsilon'], optimizer=optimizer, loss_fn=loss_fn)\n",
    "        print(f\"Model Accuracy after this action: {accuracy:.2f}\")\n",
    "\n",
    "        # Log rewards and accuracy\n",
    "        reward_history.append(reward)\n",
    "        accuracy_history.append(accuracy)\n",
    "\n",
    "        print(f\"Reward for this step: {reward}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Visualization\n",
    "    visualize_results(reward_history, accuracy_history)\n",
    "\n",
    "# Block 8: Visualization\n",
    "def visualize_results(reward_history, accuracy_history):\n",
    "    steps = list(range(1, len(reward_history) + 1))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Reward History\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, reward_history, marker='o', color='blue')\n",
    "    plt.title(\"Reward History\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Accuracy History\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, accuracy_history, marker='o', color='green')\n",
    "    plt.title(\"Accuracy History\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    rl_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bc9ba-537b-4ceb-9bb9-c9cecbe2a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# LLM Query Function\n",
    "def query_llm(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful RPG game assistant specializing in Machine Learning.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "# Step 1: Generate Corrupted Data\n",
    "def generate_corrupted_data():\n",
    "    data = {\n",
    "        'Temperature': [30, None, 25, 35, None, 28, 40],\n",
    "        'Humidity': [70, 65, None, 80, 75, None, 60],\n",
    "        'Rain': [1, 0, 1, None, 0, 1, 0]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Preprocess the Data\n",
    "def clean_data(df):\n",
    "    df = df.fillna(df.mean())  # Fill missing values with mean\n",
    "    return df\n",
    "\n",
    "# Step 3: Train Logistic Regression\n",
    "def train_model(df):\n",
    "    X = df[['Temperature', 'Humidity']]\n",
    "    y = df['Rain']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return model, accuracy, report\n",
    "\n",
    "# Main RPG Game Flow\n",
    "def play_game():\n",
    "    print(query_llm(\"Start the RPG game and introduce a scenario involving a corrupted weather scroll.\"))\n",
    "\n",
    "    data = generate_corrupted_data()\n",
    "    print(\"Corrupted Data:\\n\", data)\n",
    "\n",
    "    user_action = input(\"What would you like to do? (Options: 'clean', 'ask')\\n\")\n",
    "    if user_action.lower() == \"clean\":\n",
    "        print(query_llm(\"Explain the process of cleaning data in Machine Learning.\"))\n",
    "        cleaned_data = clean_data(data)\n",
    "        print(\"Cleaned Data:\\n\", cleaned_data)\n",
    "    elif user_action.lower() == \"ask\":\n",
    "        question = input(\"Ask the LLM a question about the data or Machine Learning: \")\n",
    "        print(query_llm(question))\n",
    "        return\n",
    "\n",
    "    print(query_llm(\"Explain the importance of training an ML model after data cleaning.\"))\n",
    "    print(\"Training a Logistic Regression model...\")\n",
    "    model, accuracy, report = train_model(cleaned_data)\n",
    "    print(\"Model trained! Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    print(query_llm(f\"Explain the results of this model's accuracy ({accuracy:.2f}) and its classification report:\\n{report}\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    play_game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4c50d-2787-44f9-b4de-74a9d1077d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "# Block 1: Dataset Generation\n",
    "def generate_dataset():\n",
    "    np.random.seed(42)\n",
    "    data = pd.DataFrame({\n",
    "        'Feature_A': np.random.rand(100) * 100,\n",
    "        'Feature_B': np.random.rand(100) * 100 + 10,\n",
    "        'Feature_C': np.random.rand(100) * 100 - 5,\n",
    "        'Target': np.random.choice([0, 1], 100)\n",
    "    })\n",
    "    data['Feature_B'] += data['Feature_A'] * 0.8  # Correlation\n",
    "    return data\n",
    "\n",
    "# Block 2: PyTorch Model Definition and Training\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_dim, 64)\n",
    "        self.layer2 = torch.nn.Linear(64, 32)\n",
    "        self.output = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return torch.sigmoid(self.output(x))\n",
    "\n",
    "def train_model(data, test_size=0.2, learning_rate=0.001, epsilon=1e-4):\n",
    "    X = data.drop(columns=['Target']).values\n",
    "    y = data['Target'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    model = SimpleNN(X_train.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss.item() < epsilon:\n",
    "            break\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        predicted = (output > 0.5).float()\n",
    "        accuracy = (predicted == y_test).float().mean().item()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Block 3: Outlier Detection\n",
    "def detect_outliers(data):\n",
    "    z_scores = np.abs(stats.zscore(data.drop(columns=['Target'])))\n",
    "    outliers = np.where(z_scores > 3)  # Flag values with z-score > 3\n",
    "    return outliers\n",
    "\n",
    "# Block 4: RL Agent Actions\n",
    "def agent_action(data, action, params):\n",
    "    feedback = \"\"\n",
    "    reward = 0  # Initialize reward\n",
    "\n",
    "    if action == \"remove_correlated\":\n",
    "        corr_matrix = data.corr()\n",
    "        to_remove = [col for col in corr_matrix if corr_matrix['Feature_A'][col] > 0.8 and col != 'Feature_A']\n",
    "        if to_remove:\n",
    "            data = data.drop(columns=to_remove)\n",
    "            feedback = f\"Removed highly correlated features: {to_remove}.\"\n",
    "            reward = 10  # Reward for reducing redundancy\n",
    "    elif action == \"apply_pca\":\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(data.drop(columns=['Target']))\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_data = pca.fit_transform(scaled_data)\n",
    "        data = pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\n",
    "        data['Target'] = data.index.map(data['Target'])\n",
    "        feedback = \"Applied PCA, reduced features to 2 principal components.\"\n",
    "        reward = 5  # Reward for reducing dimensionality\n",
    "    elif action == \"adjust_split\":\n",
    "        params['test_size'] = random.uniform(0.1, 0.4)\n",
    "        feedback = f\"Adjusted test/train split to {params['test_size']:.2f} for better evaluation.\"\n",
    "        reward = 3  # Small reward for exploration\n",
    "    elif action == \"tune_hyperparams\":\n",
    "        params['learning_rate'] = random.uniform(0.01, 10.0)\n",
    "        params['epsilon'] = random.uniform(1e-6, 1e-2)\n",
    "        feedback = f\"Set learning rate to {params['learning_rate']:.2f} and epsilon to {params['epsilon']:.6f}.\"\n",
    "        reward = 7  # Reward for experimenting with hyperparameters\n",
    "    elif action == \"include_outliers\":\n",
    "        # First, check if outliers exist\n",
    "        outliers = detect_outliers(data)\n",
    "        if len(outliers[0]) > 0:\n",
    "            feedback = \"Outliers detected, including them in the dataset.\"\n",
    "            data = data.append(data.iloc[outliers[0]])\n",
    "            reward = -5  # Adding outliers could introduce noise, so a penalty\n",
    "        else:\n",
    "            feedback = \"No outliers detected, skipping inclusion.\"\n",
    "            reward = 0  # No reward if no outliers found\n",
    "    return data, feedback, params, reward\n",
    "\n",
    "# Block 5: LLM Query (Using Hugging Face Llama)\n",
    "def query_llm(prompt):\n",
    "    model_name = \"huggingface/llama-7b\"  # Llama model with 7 billion parameters\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=200)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Block 6: RL Loop\n",
    "def rl_loop():\n",
    "    data = generate_dataset()\n",
    "    print(\"Initial Dataset:\\n\", data.head())\n",
    "\n",
    "    actions = [\"remove_correlated\", \"apply_pca\", \"adjust_split\", \"tune_hyperparams\", \"include_outliers\"]\n",
    "    params = {'test_size': 0.2, 'learning_rate': 0.001, 'epsilon': 1e-4}\n",
    "\n",
    "    reward_history = []\n",
    "    accuracy_history = []\n",
    "\n",
    "    for step in range(10):  # Simulate 10 RL steps\n",
    "        action = random.choice(actions)\n",
    "        print(f\"Step {step + 1}: Agent chose action: {action}\")\n",
    "        \n",
    "        data, feedback, params, reward = agent_action(data, action, params)\n",
    "\n",
    "        print(\"Action Feedback:\", feedback)\n",
    "        llm_response = query_llm(f\"Explain the impact of this action: {feedback}\")\n",
    "        print(\"LLM Response:\", llm_response)\n",
    "\n",
    "        accuracy = train_model(data, test_size=params['test_size'], learning_rate=params['learning_rate'], epsilon=params['epsilon'])\n",
    "        print(f\"Model Accuracy after this action: {accuracy:.2f}\")\n",
    "\n",
    "        # Log rewards and accuracy\n",
    "        reward_history.append(reward)\n",
    "        accuracy_history.append(accuracy)\n",
    "\n",
    "        print(f\"Reward for this step: {reward}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Visualization\n",
    "    visualize_results(reward_history, accuracy_history)\n",
    "\n",
    "# Block 7: Visualization\n",
    "def visualize_results(reward_history, accuracy_history):\n",
    "    steps = list(range(1, len(reward_history) + 1))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Reward History\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, reward_history, marker='o', color='blue')\n",
    "    plt.title(\"Reward History\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Accuracy History\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(steps, accuracy_history, marker='o', color='green')\n",
    "    plt.title(\"Accuracy History\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    rl_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdde46d-404d-47bf-ad92-02ab21e3c71c",
   "metadata": {},
   "source": [
    "Proof of Concept Paper: Leveraging RPG Mechanics and AI for Gamified Machine Learning Education\n",
    "Abstract\n",
    "Machine learning (ML) concepts, particularly in supervised learning, can be challenging to grasp for beginners. To overcome these barriers, we propose a novel approach that combines role-playing game (RPG) mechanics with interactive ML problem-solving, using reinforcement learning (RL) agents and large language models (LLMs) like Llama. In this framework, players assume the role of a \"data scientist adventurer\" navigating through a fictional world where they solve ML tasks and unlock story progression. The RL agents interact with dynamic datasets, optimize hyperparameters, clean noisy data, and select appropriate models. The LLM provides contextual feedback, enhancing the learning process by explaining decision-making processes. The paper outlines the design, methodology, and future research directions for this gamified learning environment.\n",
    "\n",
    "1. Introduction\n",
    "The need for innovative educational approaches to teach complex machine learning (ML) concepts is evident in the growing demand for data scientists. Traditional methods of teaching ML often present abstract, theoretical material that can be difficult for beginners to engage with. In contrast, the integration of gamified learning offers an interactive and immersive environment that enhances the learning experience. This paper introduces a proof-of-concept for a role-playing game (RPG) where players, acting as \"data scientist adventurers,\" tackle ML tasks to progress through a storyline. By leveraging reinforcement learning (RL) agents and large language models (LLMs) such as Hugging Face's Llama, the game provides dynamic feedback and guidance on decision-making, model selection, and hyperparameter tuning.\n",
    "\n",
    "The core idea is to gamify the process of solving supervised learning tasks, where players face challenges such as data cleaning, feature selection, and model optimization. As players progress through the game, they unlock new skills, tools, and datasets, learning how to apply ML techniques in a hands-on, immersive environment.\n",
    "\n",
    "2. Problem Statement\n",
    "Machine learning education is often abstract and inaccessible, particularly for beginners who struggle with theoretical concepts and mathematical foundations. Additionally, most learning platforms do not engage students in a way that connects theoretical knowledge with real-world applications. We aim to address these challenges by creating a gamified learning experience that allows players to learn ML through interactive problem-solving. The game helps players understand core ML concepts such as supervised learning, feature engineering, model evaluation, and hyperparameter tuning, while providing immediate feedback on their choices.\n",
    "\n",
    "Furthermore, incorporating RL agents into the framework allows for experimentation with different approaches, making the process of learning dynamic and adaptive. The integration of LLMs such as Llama serves to guide the player, providing explanations and context for each decision.\n",
    "\n",
    "3. Proposed Solution\n",
    "The proposed solution is a gamified, narrative-driven RPG that teaches machine learning concepts through interactive problem-solving. In this game, players assume the role of different characters with distinct abilities:\n",
    "\n",
    "The Data Alchemist: Specializes in transforming raw data into usable features through data preprocessing and feature engineering.\n",
    "The Model Wizard: Focuses on selecting algorithms, tuning hyperparameters, and optimizing models for accuracy.\n",
    "The Data Explorer: Excels at exploratory data analysis (EDA) and visualizing data to gain insights.\n",
    "The Debugging Knight: Specializes in identifying issues in data pipelines, debugging models, and ensuring data quality.\n",
    "These characters work together to solve quests based on real-world ML problems such as classification, regression, and data cleaning. The game's world is set in a fictional kingdom where datasets are represented as \"ancient scrolls,\" which are corrupted (noisy data) or locked behind ML problems.\n",
    "\n",
    "4. System Design\n",
    "4.1 Game Mechanics\n",
    "The game uses traditional RPG mechanics adapted to ML problem-solving. These include:\n",
    "\n",
    "Levels and Quests: Players progress through levels by solving ML problems, such as predicting harvest yields using weather data or optimizing a recommendation system. Each quest presents a unique challenge, requiring players to apply different ML techniques.\n",
    "Resource Management: Players manage resources such as \"mana\" or \"energy\" to perform computationally expensive tasks, such as running grid searches or training complex models.\n",
    "Skill Trees: Players unlock new skills as they progress, including advanced techniques like ensemble methods, feature selection, and hyperparameter tuning.\n",
    "Combat System: Instead of traditional combat, players face problem-solving challenges. NPCs (non-playable characters) present problems such as imbalanced datasets, and players must use tools like SMOTE (Synthetic Minority Over-sampling Technique) to resolve them.\n",
    "4.2 Reinforcement Learning (RL) Framework\n",
    "RL agents are tasked with exploring different approaches to solving ML problems, such as selecting optimizers, detecting outliers, and tuning hyperparameters. These agents experiment with strategies like choosing between Adam or SGD optimizers and deciding which features to include or exclude. The agent receives feedback based on the success of their choices in solving the quests. Additionally, the LLM provides dynamic, context-sensitive responses to explain the outcomes of decisions, helping the RL agent learn from past experiences.\n",
    "\n",
    "Outlier Detection: The RL agent is trained to detect and decide whether to include or exclude outliers in the dataset, learning through feedback provided by the LLM.\n",
    "Optimizer and Loss Function Selection: The RL agent explores different optimizer choices (e.g., Adam, RMSProp) and loss functions (e.g., binary cross-entropy, mean squared error), receiving rewards based on the accuracy of the models they generate.\n",
    "4.3 LLM Integration\n",
    "LLMs such as Llama are integrated into the game to provide explanations and insights into the choices made by the RL agents. After each decision, the LLM generates a natural language explanation of the consequences of the selected optimizer or loss function, helping players (or RL agents) understand the rationale behind their choices.\n",
    "\n",
    "5. Evaluation\n",
    "5.1 User Studies\n",
    "To evaluate the effectiveness of the game as an educational tool, we will conduct user studies to measure learning outcomes, engagement, and motivation. Participants will engage with the game and complete various ML-related quests. Pre- and post-tests will be used to measure improvements in ML knowledge, while qualitative feedback will assess the overall game experience.\n",
    "\n",
    "5.2 Benchmarking\n",
    "We will benchmark the game’s effectiveness against other ML education methods, such as online courses and tutorials. Metrics such as completion time, accuracy, and player feedback will be compared to determine whether the game provides a more engaging and effective learning experience.\n",
    "\n",
    "5.3 Qualitative Analysis\n",
    "Player feedback will be collected to gauge the user experience. This feedback will inform future iterations of the game, helping to refine the gameplay mechanics, difficulty levels, and educational content.\n",
    "\n",
    "6. Results\n",
    "We will present data from the user studies, including improvements in learning outcomes (e.g., accuracy on ML tasks), user engagement metrics (e.g., time spent playing, quests completed), and qualitative feedback on the game's impact on players’ understanding of ML concepts.\n",
    "\n",
    "7. Conclusion and Future Work\n",
    "This paper presents a novel gamified approach to teaching machine learning, using RPG mechanics and LLMs to create an immersive and interactive learning environment. By integrating RL agents into the game, we offer a dynamic, adaptive framework for solving ML problems. Future work will focus on expanding the game to include unsupervised learning tasks, enhancing the adaptiveness of LLM responses, and improving game mechanics for deeper engagement.\n",
    "\n",
    "8. Future Research Directions\n",
    "As the field of reinforcement learning evolves, future work could explore the use of this gamified environment as a training tool for RL agents. By embedding causal relationships in the game’s tasks, agents could learn to make decisions based on cause-effect reasoning rather than simple correlation-based learning. This would open up new possibilities for advancing RL research and developing intelligent systems that better understand the causal structure of the world.\n",
    "\n",
    "Additionally, the integration of counterfactual exploration and reward structuring based on causal inference could enhance the learning experience for both human players and RL agents. This research could contribute to advancing causal RL, a rapidly growing field in artificial intelligence.\n",
    "\n",
    "9. Tools and Frameworks\n",
    "The technical aspects of the game include:\n",
    "\n",
    "LLMs: Llama model from Hugging Face for generating dynamic content and providing explanations.\n",
    "ML Pipelines: Supervised learning tasks embedded in the game (e.g., classification, regression, feature selection).\n",
    "Game Development: Python-based development using libraries such as PyGame or Unity for integration with ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db1bef-1433-4763-85f3-0d613695e6e7",
   "metadata": {},
   "source": [
    "## Block 1: Environment with ML Tasks (Model Training & Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f8681-208d-435d-ab02-f155c81bee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# OpenAI API key\n",
    "openai.api_key = \"your-api-key-here\"\n",
    "\n",
    "class RPGEnvironmentWithMLTasks:\n",
    "    def __init__(self, max_steps=10):\n",
    "        self.state = None  # Current state\n",
    "        self.step_count = 0  # Steps in the episode\n",
    "        self.max_steps = max_steps  # Maximum steps per episode\n",
    "        self.hidden_dependency = \"clean missing values\"  # Causal dependency\n",
    "        self.action_log = []  # Track actions for causality evaluation\n",
    "        self.dataset = self.generate_dataset()\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        \"\"\"\n",
    "        Generate a simple dataset to simulate an ML task.\n",
    "        \"\"\"\n",
    "        X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "        y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + np.random.randn(100)  # Linear relation with noise\n",
    "        return X, y\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to start a new episode.\n",
    "        \"\"\"\n",
    "        self.step_count = 0\n",
    "        self.state = \"You are a data scientist adventurer starting your journey. Your task is to train a model to predict outcomes. What will you do first?\"\n",
    "        self.action_log = []\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute an action, transition to the next state, and return reward.\n",
    "        The action could be related to cleaning data, choosing a model, or tuning hyperparameters.\n",
    "        \"\"\"\n",
    "        self.step_count += 1\n",
    "        self.action_log.append(action)\n",
    "\n",
    "        if action == \"clean missing values\":\n",
    "            self.dataset = self.clean_data(self.dataset)\n",
    "            llm_response = f\"The dataset has been cleaned, and missing values have been handled.\"\n",
    "            reward = 10  # Reward for cleaning the dataset\n",
    "        elif action == \"select a regression model\":\n",
    "            llm_response = f\"A regression model has been selected.\"\n",
    "            reward = 5  # Reward for model selection\n",
    "        elif action == \"tune hyperparameters\":\n",
    "            model = LinearRegression()\n",
    "            # Simulate hyperparameter tuning (just random for now)\n",
    "            best_params = self.tune_hyperparameters(model)\n",
    "            llm_response = f\"Hyperparameters have been tuned. Best model found with parameters {best_params}.\"\n",
    "            reward = 15  # Reward for tuning hyperparameters\n",
    "        else:\n",
    "            llm_response = f\"Invalid action.\"\n",
    "            reward = -1  # Penalty for invalid actions\n",
    "\n",
    "        # Simulate model evaluation\n",
    "        if action == \"evaluate model\":\n",
    "            model = self.train_model()\n",
    "            performance = self.evaluate_model(model)\n",
    "            llm_response += f\" The model performance is {performance:.2f}.\"\n",
    "            reward = performance * 10  # Reward based on model performance\n",
    "\n",
    "        # Episode termination condition\n",
    "        done = self.step_count >= self.max_steps or \"end\" in llm_response.lower()\n",
    "\n",
    "        # Update state\n",
    "        self.state = llm_response\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def clean_data(self, dataset):\n",
    "        \"\"\"\n",
    "        Simulate cleaning missing data by filling NaNs with the column mean.\n",
    "        \"\"\"\n",
    "        X, y = dataset\n",
    "        X[np.random.randint(0, 100, 10), np.random.randint(0, 3, 10)] = np.nan  # Introducing some missing values\n",
    "        # Fill missing values with column mean\n",
    "        for i in range(X.shape[1]):\n",
    "            col_mean = np.nanmean(X[:, i])\n",
    "            X[np.isnan(X[:, i]), i] = col_mean\n",
    "        return X, y\n",
    "\n",
    "    def tune_hyperparameters(self, model):\n",
    "        \"\"\"\n",
    "        Simulate hyperparameter tuning.\n",
    "        \"\"\"\n",
    "        # Simulating hyperparameter tuning (returning a mock value)\n",
    "        return {\"learning_rate\": random.choice([0.01, 0.1, 0.2])}\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train a model on the dataset.\n",
    "        \"\"\"\n",
    "        X, y = self.dataset\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        Evaluate the model using RMSE (Root Mean Squared Error).\n",
    "        \"\"\"\n",
    "        X, y = self.dataset\n",
    "        predictions = model.predict(X)\n",
    "        rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "        return 1 / (1 + rmse)  # Inverse of RMSE as a performance metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e597d4d-9b48-43fb-9026-6fe522701b5b",
   "metadata": {},
   "source": [
    "## Block 2: RL Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02cc87-b53e-4465-8743-c85949699489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions  # List of possible actions\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Randomly choose an action (for now).\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.actions)\n",
    "\n",
    "# Define possible actions related to ML tasks\n",
    "actions = [\n",
    "    \"clean missing values\",\n",
    "    \"select a regression model\",\n",
    "    \"tune hyperparameters\",\n",
    "    \"evaluate model\"\n",
    "]\n",
    "\n",
    "# Initialize agent\n",
    "agent = SimpleAgent(actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d9c92-e389-4e8c-ad64-3bf26a0573b0",
   "metadata": {},
   "source": [
    "## Block 3: Running Episodes (Agent and Environment Interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8120d02-11f2-4fb5-9a06-d61bd80dcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = RPGEnvironmentWithMLTasks(max_steps=10)\n",
    "\n",
    "# Run multiple episodes\n",
    "num_episodes = 5\n",
    "episode_rewards = []  # Track rewards per episode\n",
    "causal_sequences = []  # Track action sequences for causal analysis\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(f\"\\n--- Starting Episode {episode + 1} ---\")\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    action_sequence = []\n",
    "\n",
    "    while not done:\n",
    "        print(f\"Current State: {state}\")\n",
    "        \n",
    "        # Agent selects an action\n",
    "        action = agent.choose_action(state)\n",
    "        action_sequence.append(action)\n",
    "        print(f\"Agent chooses action: {action}\")\n",
    "        \n",
    "        # Environment responds\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(f\"Reward: {reward}\")\n",
    "\n",
    "    # Track rewards and sequences\n",
    "    episode_rewards.append(total_reward)\n",
    "    causal_sequences.append(action_sequence)\n",
    "    print(f\"Episode {episode + 1} finished with Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c574c82-3dd8-472e-829b-b263a89283b1",
   "metadata": {},
   "source": [
    "## Acknowledgments Section (Example)\n",
    "“The authors would like to acknowledge OpenAI's ChatGPT for its substantial contribution in conceptualizing and generating the initial prototype for the LLM-powered RPG environment and RL agent integration. ChatGPT provided foundational code and ideas that were refined and expanded upon by the authors.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fce47-0567-48d9-ba15-cac3739eb94b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (poetry_python3)",
   "language": "python",
   "name": "poetry_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
